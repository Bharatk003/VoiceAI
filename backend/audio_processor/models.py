from django.db import models

# Create your models here.
# audio_processor/models.py

from django.db import models
from django.conf import settings # To get the AUTH_USER_MODEL for ForeignKey

class Session(models.Model):
    """
    Represents a single audio/video file uploaded or live recording initiated by a user.
    Tracks metadata about the input media and its processing status.
    """
    user = models.ForeignKey(
        settings.AUTH_USER_MODEL, # Uses your CustomUser model
        on_delete=models.CASCADE,
        related_name='sessions',
        help_text="The user who initiated this session."
    )

    title = models.CharField(
        max_length=255,
        blank=True,
        help_text="A user-friendly title for the session (e.g., 'Physics Lecture - Chapter 5')."
    )
    original_file_name = models.CharField(
        max_length=255,
        blank=True,
        null=True,
        help_text="The original name of the uploaded file."
    )
    file_path = models.FileField(
        upload_to='user_uploads/', # Files stored in MEDIA_ROOT/user_uploads/
        blank=True,
        null=True,
        help_text="Path to the original audio/video file."
    )

    MEDIA_TYPE_CHOICES = [
        ('AUDIO', 'Audio File'),
        ('VIDEO', 'Video File'),
        ('LIVE', 'Live Recording'),
    ]
    file_type = models.CharField(
        max_length=10,
        choices=MEDIA_TYPE_CHOICES,
        default='AUDIO',
        help_text="The type of media uploaded/recorded."
    )

    duration_seconds = models.FloatField(
        null=True,
        blank=True,
        help_text="Duration of the audio/video in seconds."
    )

    upload_timestamp = models.DateTimeField(
        auto_now_add=True, # Automatically sets the timestamp when created
        help_text="When the session was initiated/file was uploaded."
    )

    PROCESSING_MODE_CHOICES = [
        ('LECTURE', 'Lecture Mode'),
        ('MEETING', 'Meeting Mode'),
        ('MEDICAL', 'Medical Notes Mode'),
        ('INTERVIEW', 'Interview Mode'),
        ('JOURNAL', 'Thought Journal Mode'),
        ('PARENTING', 'Parenting Mode'),
        # Add more modes as your application expands
    ]
    processing_mode = models.CharField(
        max_length=20,
        choices=PROCESSING_MODE_CHOICES,
        default='LECTURE', # Default to lecture mode for MVP
        help_text="The mode selected for processing this session."
    )

    PROCESSING_STATUS_CHOICES = [
        ('PENDING', 'Pending Processing'), # File uploaded, waiting to be picked up
        ('UPLOADED', 'File Uploaded (Initial)'), # Redundant but kept for conceptual mapping. Will transition to PENDING.
        ('TRANSCRIBING', 'Transcribing Audio'),
        ('TRANSCRIBED', 'Transcription Completed'),
        ('ANALYZING', 'Analyzing with LLM'),
        ('COMPLETED', 'Analysis Completed'),
        ('FAILED', 'Processing Failed'),
    ]
    status = models.CharField(
        max_length=20,
        choices=PROCESSING_STATUS_CHOICES,
        default='PENDING', # Set initial status to PENDING
        help_text="Current processing status of the session."
    )

    def __str__(self):
        return f"{self.user.username}'s session: {self.title or self.original_file_name} ({self.status})"

    class Meta:
        verbose_name = "Session"
        verbose_name_plural = "Sessions"
        ordering = ['-upload_timestamp'] # Order by most recent sessions first


class AnalysisResult(models.Model):
    """
    Stores the transcription, summary, notes, and suggestions generated from a Session.
    One-to-one relationship ensures each session has one primary analysis result.
    """
    session = models.OneToOneField(
        Session,
        on_delete=models.CASCADE,
        related_name='analysis_result', # Access from Session as session.analysis_result
        help_text="The session this analysis belongs to."
    )

    # Core LLM outputs
    transcription_text = models.TextField(
        blank=True,
        help_text="The full transcribed text from the audio/video."
    )
    summary_text = models.TextField(
        blank=True,
        help_text="The LLM-generated summary of the session."
    )
    notes_text = models.TextField(
        blank=True,
        help_text="Detailed notes generated by the LLM (e.g., key points, structured data)."
    )
    suggestions_resources_text = models.TextField(
        blank=True,
        help_text="Suggestions and external resources for detailed study."
    )

    # Metadata about the analysis process
    processed_timestamp = models.DateTimeField(
        auto_now_add=True, # Automatically sets the timestamp when first processed
        help_text="When the analysis was completed."
    )
    llm_model_used = models.CharField(
        max_length=100,
        blank=True,
        null=True,
        help_text="Name of the LLM model used for analysis (e.g., 'llama-2-7b-chat')."
    )
    prompt_template_version = models.CharField(
        max_length=50,
        blank=True,
        null=True,
        help_text="Version of the prompt template used for the LLM (e.g., 'lecture_v1.0')."
    )

    def __str__(self):
        return f"Analysis for Session: {self.session.title or self.session.original_file_name}"

    class Meta:
        verbose_name = "Analysis Result"
        verbose_name_plural = "Analysis Results"